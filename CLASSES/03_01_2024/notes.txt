SVM CLASSIFICATION

SUPORT VECTOR MACHINE (SVM)
    # MAPPING DATA TO A HIGH-DIMENSIONAL FEATURE space
    # FINDING A separator

    EXAMPLE: CHANGING THE PERSPECTIVE OF THE MAIN ANALISYS

DATA transformation:
    # WHEN YOUR DATA IS NOT SEPARABLE, AND IS LINEAR
    # AFTER MULTIPLYING IT TO X^2 (KERNELLING FUNCTION), YOU NOW HAVE YOUR DATA DISPLAYED IN A QUADRATIC FUNCTION

SVM WANT TO FIND THE BETTER hyperplane
    = THINK ABOUT IT AS A LINE THAT SEPARATES TWO GROUPS

pros and cons:

    advantages:
        - accurate high dimensional spaces
        memoey efficient

    cons:
        1 thousand rows, SVM is not efficient
        overfitting if the number of features is greater than the number samples
        not provide a probability estimate
        sensitive to scaling

    Good for image CLASSIFICATION
    text category assignment
    detecting spam
    sentiment ANALISYS
    gene expression classification

OBJECTIVE:
    # Obtain the widest possible margin

    # Hard margin classification
        - when impose that all instances are off the street on the right Side
        - 
    # Soft Margin
        - slack variable

SVM with hinge loss is the most common way to apply this classifier

Linear SVM:
    LinearSVC(C01, loss="hinge") - no transformation
    SVC(kernel = "linear")
    SGDCLASSIFIER

PAY ATTENTION TO KERNEL TRICK

C is the opposite of alpha in the linear regression
The higher the value of C, the the more important is the model function

Kernels:
    - Allows to compute a transformation without really applying it
Typical kernels:~
    - Linear
    - Polynomial
    - Gaussian RBF
    - Sigmoid

