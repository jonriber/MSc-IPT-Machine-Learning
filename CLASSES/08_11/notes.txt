Simple Linear Regression

    # Another way for model evaluation: bootstrapping technique

        * generate synthetic random data from the original data (with replacement)
        * Ressampling with replacement
        * Repeat the proccess many times
        * estimante theta regression parameter for each dataset
        * from the distribution of all datasets and thetas, obtain a confidence interval (CI), inside 95%
        * this is an option if we dont wanna use test cases

Multiple Linear Regression

    * pros for using multiple linear regression are:
        - independent variables effectiveness on prediction
        - 
    

    HOW TO ESTIMATE THETA????
        - Ordinary Least Squares
            . linear algebra operations
                . takes long time for larger datasets (10k+ rows)
        - Optimazation algorithms
            . Gradient descent
                . proper approach if you have larger datasets
                . 
    Q&A for multiple linear regression

    - how to determine wheter to use simple or multiple linear regression?
    - how many independent variables to use?
    - OVERFITTING should be avoided
    - Binary variable to numerical variable


    Hyperplan is a plan with more than 2 dimensions

    Scaling and normalization
        . min -max - features are scaled between 0 and 1 (scaling)
        . z score - features are normalized to zero mean and standard deviation 1 (normalization)
        NOTE: Should not be applied to temporal series

    # GRADIENT descent

        - generic algorithm capable of finding solutions to a wide range of problems
        - main ideia is to tune parameters iteratively in order to minimize a cost function
        - GO in the direction of the sttepest slope, by measuring a local gradient, until it reaches a zero
        gradient

        HOW IT WORKS:
        - init with a random theta
        - apply derivate equation, the result might be 0, negative or positive
        - following the last result, iteratively applying for different theta values
        - controlling a tiny parameter called learning rate, this is the tuning step of the
        gradient descent

        * if our regression is of the simple type, the theta function is quadratic, which measuring
        it is convex
        * for other types of regression with not so regular bowls, we might have global minimun and local ~
        minimum
        * it also includes the learning rate




