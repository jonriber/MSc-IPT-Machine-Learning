#%%
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
iris = load_iris()
iris.keys()
iris.target_names
#test with 0 and 2 according to Professor instructions
specie=2 #0:'setosa', 1:'versicolor', 2:'virginica'
X = iris.data[:, (2, 3)] # petal length, petal width
y = (iris.target == specie).astype(int) #iris.target == 0 setosa vs rest
plt.plot(X[:,0][y==1],X[:,1][y==1],"bo")
plt.plot(X[:,0][y!=1],X[:,1][y!=1],"ms")
plt.xlabel("$x_0$: petal length", fontsize=20)
plt.ylabel("$x_1$: petal width", fontsize=20)

#%%
#PERCEPTRON
from sklearn.linear_model import Perceptron
from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score,precision_score, recall_score
per_clf = Perceptron(max_iter=1000, tol=1e-3, random_state=42)
per_clf.fit(X, y)
y_pred = per_clf.predict(X)
print('Accuracy', accuracy_score(y, y_pred))
#%%
from sklearn.linear_model import SGDClassifier
sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42,loss="perceptron", eta0=1, learning_rate="constant", penalty=None)
sgd_clf.fit(X,y)
y_pred = sgd_clf.predict(X)
print('Accuracy', accuracy_score(y, y_pred))

#%%
sgd_clf_log = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42,loss="log_loss", eta0=1, learning_rate="constant", penalty=None)
sgd_clf_log.fit(X,y)
y_pred = sgd_clf_log.predict(X)
print('Accuracy', accuracy_score(y, y_pred))


#%%
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(solver="lbfgs", random_state=42, tol=1e-3, penalty='none')
log_reg.fit(X, y)
y_pred = log_reg.predict(X)
print('Accuracy', accuracy_score(y, y_pred))

#%%
#Check the probabilities
plt.figure()
y_prob = log_reg.predict_proba(X)
y_prob
plt.plot(y_prob[:,0],'-r',label='Probability class1')
plt.plot(y_prob[:,1],'--b',label='Probability class2')
plt.legend(loc='best')
plt.xlabel('All instances', fontsize=15)
num = sum(y==0) #ajust according to specie
plt.plot([num,num],[0,1], 'k--', linewidth=5)

#%% KERAS
#################### TENSORFLOW-KERAS
import tensorflow as tf
from tensorflow import keras
#versions
tf.__version__ #or in console: >> pip show tensorflow

#%% Let´s load the data, scale it, and slipt it into training, validation and testing
#loading the MNIST fashion dataset
fashion_mnist = keras.datasets.fashion_mnist
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()
X_train_full.shape
X_train_full.dtype #each pixel represented by 8 bits (0-255)
#scaling pixel inensities to 0-1 by dividing by 255. The output will be a float between
0 and 1
#We will further divide it into train and validation (5000 instances) datasets
X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]
X_test = X_test / 255.
#%% DIMENSIONS
X_train.shape
X_valid.shape
X_test.shape

#%% VISUALIZE DATA
#plot one instance
plt.imshow(X_train[0], cmap="binary")
plt.axis('off')
#Check the classes
y_train
#let's define the class names
class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat","Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]
#example for the first element of the dataset
y_train[0]
class_names[4]
class_names[y_train[0]]
#Let's take a look at a sample of the images in the dataset
n_rows = 4
n_cols = 10
plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))
for row in range(n_rows):
    for col in range(n_cols):
        index = n_cols * row + col
        plt.subplot(n_rows, n_cols, index + 1)
        plt.imshow(X_train[index], cmap="binary", interpolation="nearest")
        plt.axis('off')
        plt.title(class_names[y_train[index]], fontsize=12)
plt.subplots_adjust(wspace=0.2, hspace=0.5)

#%% Let’s define our ANN model using Keras
#single stack of layers, connected sequentially
model = keras.models.Sequential()
#First (INPUT) layer and convert 28x28 to 1D array 1x784 pixels
model.add(keras.layers.Flatten(input_shape=[28, 28]))
#First HIDDEN LAYER (dense layer) with 300 neurons and ReLU activation
#Each layer manages its own weight matrix, containing all the connection
#weights between neurons and their inputs
#it also manages a vector of bias term
model.add(keras.layers.Dense(300, activation="relu"))
#SECOND HIDDEN LAYER with 100 neurons with ReLU activation
model.add(keras.layers.Dense(100, activation="relu"))
#OUTPUT LAYER, with 10 neurons (one per class) using softmax activation function
#as the classes are exclusive
model.add(keras.layers.Dense(10, activation="softmax"))

#%%Clearing memory and defining model again in a more compact way
#clears global state generated by the previous model
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
# #compact way of implementing the previous model is
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, activation="relu"),
    keras.layers.Dense(100, activation="relu"),
    keras.layers.Dense(10, activation="softmax")
])
#%% 
model.summary()

#%%
# from keras.utils.visualize_util import plot
keras.utils.plot_model(model, "my_MLP fashion_mnist_model.png", show_shapes=True)
#%%
model.layers #layers and memory address where the object is stored
model.layers[1].name
model.layers[2].name
hidden1 = model.layers[1]
model.get_layer(hidden1.name) is hidden1
weights, biases = hidden1.get_weights()
weights #set randomly
weights.shape
biases #all set to 0
biases.shape

#%%TRAINING THE MODEL
##TRAINING ------------
model.compile(loss="sparse_categorical_crossentropy",optimizer="sgd",metrics=["accuracy"])
# LOSS
# - "sparse_categorical_crossentropy" is used when each instance can be classified
# as a target class [0 to 9 here], and classes are exclusive
# - "categorical_crossentropy" is used if we want the target probability per class
# - "binary_crossentropy" if we were doing binary classification (with one or
# more binary labels) and we would use sigmoide (logistic) instead of softmax
# Optimizer is the well-known sgd. Keras performs the bacpropagation algorithm
# (reverse-mode autodiff + Gradient Descent)
# METRIC used is accuracy
history = model.fit(X_train, y_train, epochs=30,validation_data=(X_valid, y_valid))
#EPOCHS: the number of epochs is the number of rounds (as we have seen in SGD before)
# the loss and the metrics are measured on the validationt set at the end of each epoch
# If the performance is much better on the training set than on the test set is because
# it is overfitting

#%% Visualization of the results and Learning curves
history.params
print(history.epoch)
history.history.keys()
history.history
#Learning curves
import pandas as pd
pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1)

#%% Making predictions on Test data
#validate on the TEST SET
model.evaluate(X_test, y_test)
#predictions
np.shape(X_test)
#testing the 3 first instances
y_test[:3] #expected
X_new = X_test[:3]
y_proba = model.predict(X_new) #probabilities of each class
y_proba.round(2) #round to 2 decimal cases
y_proba
y_pred = np.argmax(y_proba, axis=1)
y_pred


#%%  Saving and loading models 

model.save("MyMLPmodel.h5")
model = keras.models.load_model("MyMLPmodel.h5")
model.save_weights("MyMLP_weights.ckpt")
model.load_weights("MyMLP_weights.ckpt")

#%%
##CROSS-VALIDATION
#first we define a model with possible input parameters (default values)
def build_model(n_hidden=2, n_neurons=300, input_shape=[28, 28], optimizer="sgd"):
    model = keras.models.Sequential()
    model.add(keras.layers.Flatten(input_shape=input_shape))
    for layer in range(n_hidden):
        model.add(keras.layers.Dense(n_neurons, activation="relu"))
    model.add(keras.layers.Dense(10, activation="softmax"))
    optimizer = keras.optimizers.SGD()
    model.compile(loss="sparse_categorical_crossentropy",optimizer=optimizer,metrics=["accuracy"])
    return model
#then, we wrap this model in sklearn
keras_class = keras.wrappers.scikit_learn.KerasClassifier(build_model)
#testing the model without tuning
history = keras_class.fit(X_train, y_train, epochs=15,validation_data=(X_valid, y_valid))
keras_class.score(X_test, y_test)
y_proba = keras_class.predict(X_test)
#parameters for GRID search
epochs = [20,30]
n_hidden =[2]
optimizer = ['sgd','adam']
param_distribs = {
"n_hidden": n_hidden,
"epochs": epochs,
"optimizer": optimizer,
}
from sklearn.model_selection import GridSearchCV
grid = GridSearchCV(estimator = keras_class, param_grid = param_distribs,cv=2,verbose=2,)
grid_search = grid.fit(X_train, y_train,validation_data=(X_valid, y_valid))
grid_search.best_score_
grid_search.best_params_